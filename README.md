# ChatTogoVar: A TogoVar-based Retrieval-Augmented Generation System for Precise Genomic Variant Interpretation

This repository contains the source code, inputs, and outputs used in the study, including:
- Question templates and rs-number selection
- Answer generation pipelines (ChatTogoVar, GPT-4o, VarChat)
- Manual (human) and LLM-based evaluation results
- Statistical analyses and scripts to generate Tables 3–5 (with confidence intervals and significance tests)
- A public demo website deployment using a self-hosted LLM

## Key directories and files
- `answers/`  Generated answers (Markdown) for each system/model.
- `.env.template` Template for required environment variables (e.g., Azure OpenAI endpoint/key/deployment name). Create a local `.env` from this template; `.env` is not committed.
- `evaluation/` Evaluation inputs/outputs and table artifacts.
  - `human-primary/` Manual evaluation results (primary evaluator).
  - `audit/` Differences before vs. after the secondary audit (performed by the secondary reviewer).
  - `human-final/` Audited manual evaluation results.
  - `gpt-4o/` LLM-based evaluation results generated by GPT-4o as an evaluator.
  - `tables/` Final Table 3–5 outputs generated by scripts.
- `pipeline/` Shell entrypoints that run the main workflows.
  - `00_get_rs.sh` Build the curated input list of rsIDs used in the study (from PubTator3 / filtering steps).
  - `01_generate_answers.sh` Generate answer Markdown files for each question template × rsID pair using ChatTogoVar, GPT-4o, and VarChat.
  - `02_llm-based_evaluation.sh` Run GPT-4o as an automatic evaluator to score generated answers and save per-question evaluation outputs.
  - `03_audit_evaluation.sh` Perform a secondary quality-control audit of the primary manual-evaluation records (e.g., fill missing values / fix obvious recording errors).
  - `04_aggregate_evaluation.sh` Aggregate per-question evaluation JSONs into summary artifacts (e.g., aggregated JSON/TSV) used for analysis and table generation.
  - `05_compare_primary_audit.sh` Quantify agreement and changes between primary vs. audited manual ratings (e.g., ICC(2,1), weighted κ, |Δ| summaries).
  - `06_build_table_3_5.sh` Generate the final manuscript tables (Tables 3–5), including confidence intervals and statistical tests, from the aggregated results.
- `src/` Python source code invoked by the scripts in the `pipeline` directory.
- `web/` Public demo deployment at https://chattogovar.dbcls.jp (self-hosted LLM + docker-compose, etc.)

## Quick start: reproduce Tables 3–5

### 1. Azure OpenAI resource creation and model deployment

Follow Microsoft’s official instructions:
- Create and deploy an Azure OpenAI in Microsoft Foundry Models resource:
https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/create-resource

For cost control and billing configuration, see:
- Azure OpenAI pricing: https://azure.microsoft.com/en-us/pricing/details/azure-openai/
- Azure Cost Management budgets/alerts: https://learn.microsoft.com/en-us/azure/cost-management-billing/costs/tutorial-acm-create-budgets
-	Azure OpenAI quotas/limits: https://learn.microsoft.com/en-us/azure/ai-foundry/openai/quotas-limits

### 2. Create `.env` from the template and fill in Azure OpenAI settings

```
cp .env.template .env
# Edit .env and set:
#   api_key=...
#   api_base=...            # Azure OpenAI endpoint
#   deployment_name=...     # e.g., gpt-4o
```

### 3. Set up Python environment

```
$ git clone https://github.com/mitsuhashi/chattogovar
$ cd chattogovar
$ python -m venv .venv
$ source .venv/bin/activate
(.venv) pip install -U pip
(.venv) pip install -r requirements.txt
```

### 4.  Run the pipelines in order

```
(.venv) $ bash pipeline/00_get_rs.sh
(.venv) $ bash pipeline/01_generate_answers.sh
(.venv) $ bash pipeline/02_llm-based_evaluation.sh
(.venv) $ bash pipeline/03_audit_evaluation.sh
(.venv) $ bash pipeline/04_aggregate_evaluation.sh
(.venv) $ bash pipeline/05_compare_primary_audit.sh
(.venv) $ bash pipeline/06_build_table_3_5.sh
```

### 5.	Outputs

- Generated answers: answers/
- Evaluation results: evaluation/
- Final tables (Tables 3–5): evaluation/tables/ (e.g., table3.md, table4.md, table5.md)

## Quick start: public demo website (self-hosted LLM)

### 1. Backend (self-hosted LLM + docker-compose configuration)

```bash
cd web/self-hosted-llm
export HF_TOKEN="<YOUR_HUGGINGFACE_TOKEN>"
bash download_qwen.sh
docker compose up -d
```

This Docker Compose service runs the [llama.cpp](https://github.com/ggml-org/llama.cpp/pkgs/container/llama.cpp) CUDA server to serve a Qwen3 GGUF model with GPU acceleration.
  - It starts the container qwen3-llamacpp from ghcr.io/ggml-org/llama.cpp:server-cuda, restarts unless stopped, and exposes the API on localhost:18000 → container:8080.
  - It mounts ./models read-only and loads Qwen3-30B-A3B-Instruct-2507-Q5_K_M.gguf from /models.
  - It enables NVIDIA GPUs and restricts the container to GPUs 0 and 1, splitting tensors evenly across them (--tensor-split 0.5,0.5) with layer-based splitting.
  - It configures a very large context window (262,144 tokens) and uses quantized KV cache (q8_0) for both K and V to reduce memory usage while keeping performance.

### 2. Frontend (Dify workflow)

Import Dify DSL `web/workflow/ChatTogoVar.yml` to Dify. See [Creating from a DSL File](https://docs.dify.ai/versions/3-0-x/en/user-guide/application-orchestrate/creating-an-application#creating-from-a-dsl-file) for details.

Note: A reverse proxy server may be required between the frontend at Dify and the on-premise backend server.